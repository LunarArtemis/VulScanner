{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM:\n",
    "    def __init__(self, data_path, name=\"graph2vec_BLSTM\", batch_size=64, epochs=20):\n",
    "        self.name = name\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Load and preprocess data\n",
    "        self._load_data(data_path)\n",
    "\n",
    "        # Build and compile model\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _load_data(self, data_path):\n",
    "        data = pd.read_csv(data_path)  # Load graph2vec features\n",
    "        print(\"Dataset Preview:\")\n",
    "        print(data.head())\n",
    "\n",
    "        indices = data['type'].values\n",
    "        y = data.iloc[:, 4]\n",
    "        y_labels = y.iloc[indices]\n",
    "        \n",
    "        X = data.drop(columns=\"type\")\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42)\n",
    "        print(\"Train data shape:\", self.X_train.shape)\n",
    "        print(\"Test data shape:\", self.X_test.shape)\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.02), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(self.name + \"_best_model.weights.keras\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            callbacks=[early_stopping, model_checkpoint]\n",
    "        )\n",
    "\n",
    "        self._plot_learning_curve(history)\n",
    "\n",
    "    def _plot_learning_curve(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.ylim(0, 2)\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_weights(self.name + \"_best_model.weights.keras\")\n",
    "        results = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(\"Test loss:\", results[0])\n",
    "        print(\"Test accuracy:\", results[1])\n",
    "        print(\"Test AUC:\", results[2])\n",
    "\n",
    "# Usage:\n",
    "# blstm = BLSTM(\"graph2vec_features.csv\")\n",
    "# blstm.train()\n",
    "# blstm.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 54\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mGNN.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# GCN layers\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_conv \u001b[38;5;241m=\u001b[39m GCNConv(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mnum_features, embedding_size) \u001b[38;5;66;03m# To transform input features to the size of the embeddings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m GCNConv(embedding_size, embedding_size)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m GCNConv(embedding_size, embedding_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU, Sigmoid\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(32)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(dataset.num_features, embedding_size) # To transform input features to the size of the embeddings\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        \n",
    "        # Output layer\n",
    "        self.lin1 = Linear(embedding_size*2, 128) # Concatenation of global pooling results\n",
    "        self.lin2 = Linear(128, 64)\n",
    "        self.lin3 = Linear(64, 1)\n",
    "        \n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.relu(hidden)\n",
    "        \n",
    "        # Global Pooling\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], \n",
    "                            dim=1)\n",
    "        \n",
    "        # Apply some linear layers\n",
    "        out = self.lin1(hidden) \n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.lin3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = GNN()\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    \n",
    "    for data in train_dataset_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = gnn_model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.float())\n",
    "        loss.backward()\n",
    "        loss_all += loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset_loader)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            # pred = model(data.x.float(), data.edge_index, data.batch).detach().cpu().numpy()\n",
    "            pred = model(data.x.float(), data.edge_index, data.batch)\n",
    "            label_true = data.y.to(device)\n",
    "            label = data.y.detach().cpu().numpy()\n",
    "            # predictions.append(pred)\n",
    "            # labels.append(label)\n",
    "            predictions.append(np.rint(pred.cpu().detach().numpy()))\n",
    "            labels.append(label)\n",
    "            loss = loss_fn(pred.squeeze(), label_true.float())\n",
    "    # predictions = np.hstack(predictions)\n",
    "    # labels = np.hstack(labels)\n",
    "    predictions = np.concatenate(predictions).ravel()\n",
    "    labels = np.concatenate(labels).ravel()\n",
    "\n",
    "    # print(predictions)\n",
    "    # print(labels)\n",
    "    return accuracy_score(labels, predictions), loss\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_acc_list= []\n",
    "train_acc_list= []\n",
    "best_loss = 1000\n",
    "early_stopping_counter = 0\n",
    "for epoch in range(200):\n",
    "    if early_stopping_counter <=  5: # = x * 5 \n",
    "        loss = train()\n",
    "        train_losses.append(loss)\n",
    "        train_acc, train_loss = evaluate(train_dataset_loader)\n",
    "        #val_acc = evaluate(val_loader)    \n",
    "        val_acc, val_loss = evaluate(val_dataset_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "        train_acc_list.append(train_acc)\n",
    "    \n",
    "        if float(val_loss) < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # Save the currently best model \n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss} | Train Accuracy{train_acc} | Validation Accuracy{val_acc} | Validation loss{val_loss}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Early stopping due to no improvement.\")\n",
    "        break\n",
    "print(f\"Finishing training with best val loss: {best_loss}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
