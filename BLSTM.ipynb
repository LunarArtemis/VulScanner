{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, BatchNormalization, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels\n",
    "data = pd.read_csv(\"Datasets/Normalized_CWE-469.csv\")\n",
    "#get embeddings \n",
    "x= pd.read_csv(\"graph2vec/features/embledding_cwe_469.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"vuln\"] = data[\"vuln\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>testID</th>\n",
       "      <th>filename</th>\n",
       "      <th>code</th>\n",
       "      <th>vuln</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cwe469_0.c</td>\n",
       "      <td>gretl_list_build (const char *s, const DATASET...</td>\n",
       "      <td>0</td>\n",
       "      <td>CWE-469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cwe469_1.c</td>\n",
       "      <td>rd_meta_is_broken(FILE *fp)\\n{\\n    char buf[M...</td>\n",
       "      <td>1</td>\n",
       "      <td>CWE-469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>cwe469_2.c</td>\n",
       "      <td>load( f_ck_query query_func, t_CKBOOL lazy )\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>CWE-469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>cwe469_3.c</td>\n",
       "      <td>checkSupGroups (LDAP * ld)\\n{\\n  LDAPMessage *...</td>\n",
       "      <td>1</td>\n",
       "      <td>CWE-469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>cwe469_4.c</td>\n",
       "      <td>dht_getxattr_unwind (call_frame_t *frame,\\n   ...</td>\n",
       "      <td>0</td>\n",
       "      <td>CWE-469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  testID    filename  \\\n",
       "0           0       0  cwe469_0.c   \n",
       "1           1       1  cwe469_1.c   \n",
       "2           2       2  cwe469_2.c   \n",
       "3           3       3  cwe469_3.c   \n",
       "4           4       4  cwe469_4.c   \n",
       "\n",
       "                                                code  vuln     type  \n",
       "0  gretl_list_build (const char *s, const DATASET...     0  CWE-469  \n",
       "1  rd_meta_is_broken(FILE *fp)\\n{\\n    char buf[M...     1  CWE-469  \n",
       "2  load( f_ck_query query_func, t_CKBOOL lazy )\\n...     0  CWE-469  \n",
       "3  checkSupGroups (LDAP * ld)\\n{\\n  LDAPMessage *...     1  CWE-469  \n",
       "4  dht_getxattr_unwind (call_frame_t *frame,\\n   ...     0  CWE-469  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "10      0\n",
       "100     0\n",
       "1000    1\n",
       "Name: vuln, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = x['type'].values\n",
    "y = data.iloc[:, 4]\n",
    "y_labels = y.iloc[indices]\n",
    "y_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.086220</td>\n",
       "      <td>-0.183489</td>\n",
       "      <td>-0.311828</td>\n",
       "      <td>0.210561</td>\n",
       "      <td>-0.011794</td>\n",
       "      <td>0.046493</td>\n",
       "      <td>-0.247115</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>-0.054876</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136654</td>\n",
       "      <td>0.208798</td>\n",
       "      <td>0.118810</td>\n",
       "      <td>0.194054</td>\n",
       "      <td>0.098417</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.060261</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.089105</td>\n",
       "      <td>-0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050206</td>\n",
       "      <td>-0.533095</td>\n",
       "      <td>-0.311225</td>\n",
       "      <td>0.058918</td>\n",
       "      <td>-0.018652</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>-0.350888</td>\n",
       "      <td>-0.090321</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.200890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236584</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>-0.022136</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>-0.023393</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>-0.102472</td>\n",
       "      <td>-0.181720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023950</td>\n",
       "      <td>-0.167754</td>\n",
       "      <td>-0.200610</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.013925</td>\n",
       "      <td>-0.161392</td>\n",
       "      <td>-0.095699</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.130106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096194</td>\n",
       "      <td>0.076208</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.087224</td>\n",
       "      <td>0.073983</td>\n",
       "      <td>-0.024665</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>-0.029866</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>-0.076197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008526</td>\n",
       "      <td>-0.158457</td>\n",
       "      <td>-0.380494</td>\n",
       "      <td>0.172831</td>\n",
       "      <td>0.048501</td>\n",
       "      <td>-0.012732</td>\n",
       "      <td>-0.286329</td>\n",
       "      <td>-0.010031</td>\n",
       "      <td>-0.268125</td>\n",
       "      <td>0.163671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191623</td>\n",
       "      <td>0.287033</td>\n",
       "      <td>-0.020448</td>\n",
       "      <td>0.148487</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>-0.068902</td>\n",
       "      <td>-0.027892</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>-0.069266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005788</td>\n",
       "      <td>-0.437232</td>\n",
       "      <td>-0.307672</td>\n",
       "      <td>0.062768</td>\n",
       "      <td>-0.005194</td>\n",
       "      <td>-0.015667</td>\n",
       "      <td>-0.310069</td>\n",
       "      <td>-0.057550</td>\n",
       "      <td>-0.023266</td>\n",
       "      <td>0.175896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232327</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>-0.008981</td>\n",
       "      <td>0.134089</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>-0.011975</td>\n",
       "      <td>0.156055</td>\n",
       "      <td>-0.036535</td>\n",
       "      <td>-0.023201</td>\n",
       "      <td>-0.182217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_0       x_1       x_2       x_3       x_4       x_5       x_6  \\\n",
       "0 -0.086220 -0.183489 -0.311828  0.210561 -0.011794  0.046493 -0.247115   \n",
       "1  0.050206 -0.533095 -0.311225  0.058918 -0.018652 -0.024216 -0.350888   \n",
       "2 -0.023950 -0.167754 -0.200610  0.141800 -0.017827 -0.013925 -0.161392   \n",
       "3 -0.008526 -0.158457 -0.380494  0.172831  0.048501 -0.012732 -0.286329   \n",
       "4  0.005788 -0.437232 -0.307672  0.062768 -0.005194 -0.015667 -0.310069   \n",
       "\n",
       "        x_7       x_8       x_9  ...     x_118     x_119     x_120     x_121  \\\n",
       "0  0.002444 -0.054876  0.163939  ...  0.136654  0.208798  0.118810  0.194054   \n",
       "1 -0.090321  0.005125  0.200890  ...  0.236584  0.052393 -0.022136  0.186589   \n",
       "2 -0.095699  0.004724  0.130106  ...  0.096194  0.076208  0.018431  0.087224   \n",
       "3 -0.010031 -0.268125  0.163671  ...  0.191623  0.287033 -0.020448  0.148487   \n",
       "4 -0.057550 -0.023266  0.175896  ...  0.232327  0.029678 -0.008981  0.134089   \n",
       "\n",
       "      x_122     x_123     x_124     x_125     x_126     x_127  \n",
       "0  0.098417 -0.000896 -0.060261  0.012516  0.089105 -0.105244  \n",
       "1  0.007961 -0.023393  0.154599 -0.031407 -0.102472 -0.181720  \n",
       "2  0.073983 -0.024665 -0.001303 -0.029866  0.062740 -0.076197  \n",
       "3  0.040430 -0.068902 -0.027892  0.025348  0.106796 -0.069266  \n",
       "4  0.028003 -0.011975  0.156055 -0.036535 -0.023201 -0.182217  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.drop(columns=\"type\")\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([x, y_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>vuln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.086220</td>\n",
       "      <td>-0.183489</td>\n",
       "      <td>-0.311828</td>\n",
       "      <td>0.210561</td>\n",
       "      <td>-0.011794</td>\n",
       "      <td>0.046493</td>\n",
       "      <td>-0.247115</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>-0.054876</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208798</td>\n",
       "      <td>0.118810</td>\n",
       "      <td>0.194054</td>\n",
       "      <td>0.098417</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.060261</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.089105</td>\n",
       "      <td>-0.105244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050206</td>\n",
       "      <td>-0.533095</td>\n",
       "      <td>-0.311225</td>\n",
       "      <td>0.058918</td>\n",
       "      <td>-0.018652</td>\n",
       "      <td>-0.024216</td>\n",
       "      <td>-0.350888</td>\n",
       "      <td>-0.090321</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.200890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>-0.022136</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>-0.023393</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>-0.102472</td>\n",
       "      <td>-0.181720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023950</td>\n",
       "      <td>-0.167754</td>\n",
       "      <td>-0.200610</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.013925</td>\n",
       "      <td>-0.161392</td>\n",
       "      <td>-0.095699</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.130106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076208</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.087224</td>\n",
       "      <td>0.073983</td>\n",
       "      <td>-0.024665</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>-0.029866</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>-0.076197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008526</td>\n",
       "      <td>-0.158457</td>\n",
       "      <td>-0.380494</td>\n",
       "      <td>0.172831</td>\n",
       "      <td>0.048501</td>\n",
       "      <td>-0.012732</td>\n",
       "      <td>-0.286329</td>\n",
       "      <td>-0.010031</td>\n",
       "      <td>-0.268125</td>\n",
       "      <td>0.163671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287033</td>\n",
       "      <td>-0.020448</td>\n",
       "      <td>0.148487</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>-0.068902</td>\n",
       "      <td>-0.027892</td>\n",
       "      <td>0.025348</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>-0.069266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005788</td>\n",
       "      <td>-0.437232</td>\n",
       "      <td>-0.307672</td>\n",
       "      <td>0.062768</td>\n",
       "      <td>-0.005194</td>\n",
       "      <td>-0.015667</td>\n",
       "      <td>-0.310069</td>\n",
       "      <td>-0.057550</td>\n",
       "      <td>-0.023266</td>\n",
       "      <td>0.175896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>-0.008981</td>\n",
       "      <td>0.134089</td>\n",
       "      <td>0.028003</td>\n",
       "      <td>-0.011975</td>\n",
       "      <td>0.156055</td>\n",
       "      <td>-0.036535</td>\n",
       "      <td>-0.023201</td>\n",
       "      <td>-0.182217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_0       x_1       x_2       x_3       x_4       x_5       x_6  \\\n",
       "0 -0.086220 -0.183489 -0.311828  0.210561 -0.011794  0.046493 -0.247115   \n",
       "1  0.050206 -0.533095 -0.311225  0.058918 -0.018652 -0.024216 -0.350888   \n",
       "2 -0.023950 -0.167754 -0.200610  0.141800 -0.017827 -0.013925 -0.161392   \n",
       "3 -0.008526 -0.158457 -0.380494  0.172831  0.048501 -0.012732 -0.286329   \n",
       "4  0.005788 -0.437232 -0.307672  0.062768 -0.005194 -0.015667 -0.310069   \n",
       "\n",
       "        x_7       x_8       x_9  ...     x_119     x_120     x_121     x_122  \\\n",
       "0  0.002444 -0.054876  0.163939  ...  0.208798  0.118810  0.194054  0.098417   \n",
       "1 -0.090321  0.005125  0.200890  ...  0.052393 -0.022136  0.186589  0.007961   \n",
       "2 -0.095699  0.004724  0.130106  ...  0.076208  0.018431  0.087224  0.073983   \n",
       "3 -0.010031 -0.268125  0.163671  ...  0.287033 -0.020448  0.148487  0.040430   \n",
       "4 -0.057550 -0.023266  0.175896  ...  0.029678 -0.008981  0.134089  0.028003   \n",
       "\n",
       "      x_123     x_124     x_125     x_126     x_127  vuln  \n",
       "0 -0.000896 -0.060261  0.012516  0.089105 -0.105244     0  \n",
       "1 -0.023393  0.154599 -0.031407 -0.102472 -0.181720     1  \n",
       "2 -0.024665 -0.001303 -0.029866  0.062740 -0.076197     0  \n",
       "3 -0.068902 -0.027892  0.025348  0.106796 -0.069266     1  \n",
       "4 -0.011975  0.156055 -0.036535 -0.023201 -0.182217     0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "example = dataset.iloc[0]\n",
    "print(example.vuln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['vuln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "# Into 80% training and 10% testing and 10% validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length:  4252\n",
      "X_test length:  525\n",
      "X_val length:  473\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train length: \", len(X_train))\n",
    "print(\"X_test length: \", len(X_test))\n",
    "print(\"X_val length: \", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "X_train_tensors = torch.FloatTensor(X_train.values)\n",
    "X_test_tensors = torch.FloatTensor(X_test.values)\n",
    "X_val_tensors = torch.FloatTensor(X_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0962, -0.2087, -0.1513,  0.1960,  0.0422,  0.0698, -0.2391, -0.3496,\n",
      "         0.0520,  0.1334, -0.2362,  0.1998,  0.1078, -0.3431,  0.0169, -0.0645,\n",
      "         0.1341,  0.1785, -0.1672, -0.1491, -0.2454,  0.1709, -0.1057,  0.0595,\n",
      "         0.0792, -0.0384, -0.3708, -0.1085,  0.0331, -0.3148, -0.1245,  0.1213,\n",
      "        -0.0486, -0.0577,  0.2229,  0.1514,  0.0426, -0.0026, -0.0393, -0.2105,\n",
      "         0.0204, -0.1162, -0.2300, -0.2593, -0.0649,  0.0251,  0.1276, -0.0036,\n",
      "        -0.0604,  0.0435,  0.0796, -0.0301, -0.3028,  0.1963,  0.0399,  0.2024,\n",
      "        -0.0506, -0.0663,  0.0906, -0.2888, -0.2172,  0.2024, -0.0976, -0.0941,\n",
      "         0.3093,  0.2353, -0.1213,  0.0080, -0.1196, -0.2319, -0.1507,  0.0929,\n",
      "        -0.2444, -0.2500, -0.1284, -0.0671, -0.1139,  0.0286, -0.2387, -0.0669,\n",
      "        -0.2107,  0.0153, -0.1803,  0.1660, -0.0395, -0.0891,  0.1080,  0.0607,\n",
      "         0.0361,  0.0348, -0.2703,  0.4720, -0.3695, -0.0428, -0.2394,  0.1188,\n",
      "        -0.0222, -0.0077,  0.0362,  0.1691, -0.1863, -0.2437, -0.2145,  0.1760,\n",
      "        -0.1269,  0.0040,  0.1603, -0.0827,  0.0089,  0.0113, -0.1250, -0.0881,\n",
      "        -0.0738, -0.1111,  0.2208,  0.0690, -0.1964,  0.1126,  0.0683,  0.3271,\n",
      "         0.0022,  0.3050,  0.0734, -0.1992, -0.0153, -0.1381,  0.1490, -0.2804])\n"
     ]
    }
   ],
   "source": [
    "example = X_train_tensors[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensors = torch.tensor(y_train.values)\n",
    "y_test_tensors = torch.tensor(y_test.values)\n",
    "y_val_tensors = torch.tensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "example = y_train_tensors[399]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6949, Val Loss: 0.6992\n",
      "Epoch [2/20], Loss: 0.6926, Val Loss: 0.6976\n",
      "Epoch [3/20], Loss: 0.6895, Val Loss: 0.6970\n",
      "Epoch [4/20], Loss: 0.6882, Val Loss: 0.6986\n",
      "Epoch [5/20], Loss: 0.6876, Val Loss: 0.6941\n",
      "Epoch [6/20], Loss: 0.6858, Val Loss: 0.7051\n",
      "Epoch [7/20], Loss: 0.6844, Val Loss: 0.7050\n",
      "Epoch [8/20], Loss: 0.6813, Val Loss: 0.7042\n",
      "Epoch [9/20], Loss: 0.6823, Val Loss: 0.6990\n",
      "Epoch [10/20], Loss: 0.6791, Val Loss: 0.7076\n",
      "Epoch [11/20], Loss: 0.6780, Val Loss: 0.7107\n",
      "Epoch [12/20], Loss: 0.6758, Val Loss: 0.7070\n",
      "Epoch [13/20], Loss: 0.6727, Val Loss: 0.7128\n",
      "Epoch [14/20], Loss: 0.6724, Val Loss: 0.7147\n",
      "Epoch [15/20], Loss: 0.6671, Val Loss: 0.7159\n",
      "Epoch [16/20], Loss: 0.6740, Val Loss: 0.7131\n",
      "Epoch [17/20], Loss: 0.6668, Val Loss: 0.7152\n",
      "Epoch [18/20], Loss: 0.6657, Val Loss: 0.7082\n",
      "Epoch [19/20], Loss: 0.6628, Val Loss: 0.7149\n",
      "Epoch [20/20], Loss: 0.6631, Val Loss: 0.7181\n",
      "Test Loss: 0.7241\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Fix target tensor type\n",
    "y_train_tensors = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val_tensors = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test_tensors = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensors, y_train_tensors.unsqueeze(1))\n",
    "val_dataset = TensorDataset(X_val_tensors, y_val_tensors.unsqueeze(1))\n",
    "test_dataset = TensorDataset(X_test_tensors, y_test_tensors.unsqueeze(1))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# **Improved MLP Model**\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),  # Normalize input\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Increase dropout\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize Model\n",
    "input_dim = X_train_tensors.shape[1]\n",
    "mlp_model = MLP(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 Regularization\n",
    "\n",
    "# **Training Function**\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# **Evaluation Function**\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# **Train & Test Model**\n",
    "train_model(mlp_model, train_loader, val_loader, criterion, optimizer, epochs=20)\n",
    "test_loss = evaluate_model(mlp_model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
